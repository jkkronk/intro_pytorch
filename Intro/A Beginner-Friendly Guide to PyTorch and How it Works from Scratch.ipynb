{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Beginner-Friendly Guide to PyTorch and How it Works from Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow blog post here: https://www.analyticsvidhya.com/blog/2019/09/introduction-to-pytorch-from-scratch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and install Pytorch from here: https://pytorch.org/get-started/locally/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction to Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch tensors is very similar to numpy arrays. We initialize them as follows but can also use torch.ones(), torch.zeros() and random as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> \n",
      " tensor([[3, 3, 3],\n",
      "        [2, 2, 2],\n",
      "        [1, 1, 1]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[3,3,3],[2,2,2],[1,1,1]])\n",
    "print(type(a),'\\n', a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "az = torch.zeros((3,3))\n",
    "print(az)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.],\n",
      "        [1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "ao = torch.ones((3,3))\n",
    "print(ao)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3923, -0.2236, -0.3195],\n",
      "        [-1.2050,  1.0445, -0.6332],\n",
      "        [ 0.5731,  0.5409, -0.3919]])\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(2)\n",
    "ar = torch.randn(3,3)\n",
    "print(ar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Matrix operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is done similarly to numpy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ar+ao:  tensor([[ 1.3923,  0.7764,  0.6805],\n",
      "        [-0.2050,  2.0445,  0.3668],\n",
      "        [ 1.5731,  1.5409,  0.6081]])\n",
      "ar-ao:  tensor([[-0.6077, -1.2236, -1.3195],\n",
      "        [-2.2050,  0.0445, -1.6332],\n",
      "        [-0.4269, -0.4591, -1.3919]])\n",
      "ar*ao:  tensor([[-0.1508, -0.1508, -0.1508],\n",
      "        [-0.7938, -0.7938, -0.7938],\n",
      "        [ 0.7221,  0.7221,  0.7221]])\n",
      "ar/ao:  tensor([[ 0.3923, -0.2236, -0.3195],\n",
      "        [-1.2050,  1.0445, -0.6332],\n",
      "        [ 0.5731,  0.5409, -0.3919]])\n"
     ]
    }
   ],
   "source": [
    "print('ar+ao: ',torch.add(ar,ao))\n",
    "print('ar-ao: ',torch.sub(ar,ao))\n",
    "print('ar*ao: ',torch.mm(ar,ao))\n",
    "print('ar/ao: ',torch.div(ar,ao))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Transposing tensors</b>  is done with torch.t()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[3, 3, 3],\n",
      "        [2, 2, 2],\n",
      "        [1, 1, 1]])\n",
      "\n",
      " transpose(a) = \n",
      " tensor([[3, 2, 1],\n",
      "        [3, 2, 1],\n",
      "        [3, 2, 1]])\n"
     ]
    }
   ],
   "source": [
    "print('a =',a)\n",
    "at = torch.t(a)\n",
    "print('\\n transpose(a) = \\n', at)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use torch.cat((a,b)) to <b>Concatenting tensors </b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,2))\n",
    "b = torch.ones((2,2))\n",
    "\n",
    "cat_ab = torch.cat((a,b))\n",
    "print(cat_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use change dim if we want to concatenate the tensors horizontally. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n"
     ]
    }
   ],
   "source": [
    "cat_ab = torch.cat((a,b), dim=1)\n",
    "print(cat_ab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Reshaping tensors </b>, similarly to numpy is done by .reshape() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6147, 0.3810, 0.6371],\n",
      "        [0.4745, 0.7136, 0.6190],\n",
      "        [0.4425, 0.0958, 0.6142]]) torch.Size([3, 3])\n",
      "tensor([[0.6147, 0.3810, 0.6371, 0.4745, 0.7136, 0.6190, 0.4425, 0.0958, 0.6142]]) torch.Size([1, 9])\n"
     ]
    }
   ],
   "source": [
    "# setting random seed\n",
    "torch.manual_seed(2)\n",
    "\n",
    "# init tensor\n",
    "a = torch.rand(3,3)\n",
    "print(a, a.shape)\n",
    "\n",
    "# reshaping tensor with .reshape()\n",
    "a_reshaped = a.reshape(1,9)\n",
    "print(a_reshaped, a_reshaped.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common PyTorch Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Autograd Module\n",
    "PyTorch uses a technique called automatic differentiation. It records all the operations that we are performing and replays it backward to compute gradients. This technique helps us to save time on each epoch as we are calculating the gradients on the forward pass itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init tensor\n",
    "a = torch.ones((3,3), requires_grad=True)\n",
    "\n",
    "# perform operations\n",
    "b = a + 2\n",
    "c = b.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that c = sum(a+2)/9 and by manually calulate the gradients it would be 1/9=0.111... We verify this using pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1111, 0.1111, 0.1111],\n",
      "        [0.1111, 0.1111, 0.1111],\n",
      "        [0.1111, 0.1111, 0.1111]])\n"
     ]
    }
   ],
   "source": [
    "c.backward()\n",
    "print(a.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optim Module\n",
    "The Optim module in PyTorch has pre-written codes for most of the optimizers that are used while building a neural network. We just have to import them and then they can be used to build models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the optim module\n",
    "from torch import optim\n",
    "\n",
    "# adam\n",
    "## adam = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pytorch have support for most commonly used optimizers including: \n",
    " - SGD\n",
    " - ADAM\n",
    " - Adadelta\n",
    " - Adagrad\n",
    " - AdamW\n",
    " - SparseAdam\n",
    " - Adamax\n",
    " - RMSprop\n",
    " - And more..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Neural Network from Scratch in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we init model parameters we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input tensor\n",
    "X = torch.Tensor([[1,0,1,0],[1,0,1,1],[0,1,0,1]])\n",
    "\n",
    "# output tensor \n",
    "y = torch.Tensor([[1],[1],[0]])\n",
    "\n",
    "# define activation function sigmoid and derivative of the \n",
    "# sigmoid for backpropagation\n",
    "def sigmoid(x):\n",
    "    return 1/(1 + torch.exp(-x))\n",
    "\n",
    "def der_sigmoid(x):\n",
    "    return x * (1-x)\n",
    "\n",
    "# init variables \n",
    "epoch = 7000\n",
    "lr_rate = 0.01\n",
    "X_neurons = X.shape[1] # nbr of features in data set\n",
    "hidden_neurons = 3 # nbr of hidden layers\n",
    "Y_neurons = 1 # nbr of output layers\n",
    "\n",
    "# init weights and bias, we randomly init weights and biases \n",
    "wh=torch.randn(X_neurons, hidden_neurons).type(torch.FloatTensor)\n",
    "bh=torch.randn(1, hidden_neurons).type(torch.FloatTensor)\n",
    "wout=torch.randn(hidden_neurons, Y_neurons)\n",
    "bout=torch.randn(1, Y_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will create a neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epoch):\n",
    "    #Forward Propogation\n",
    "    hidden_layer_input1 = torch.mm(X, wh)\n",
    "    hidden_layer_input = hidden_layer_input1 + bh\n",
    "    hidden_layer_activations = sigmoid(hidden_layer_input)\n",
    "\n",
    "    output_layer_input1 = torch.mm(hidden_layer_activations, wout)\n",
    "    output_layer_input = output_layer_input1 + bout\n",
    "    output = sigmoid(output_layer_input)\n",
    "\n",
    "    #Backpropagation\n",
    "    E = y-output\n",
    "    slope_output_layer = der_sigmoid(output)\n",
    "    slope_hidden_layer = der_sigmoid(hidden_layer_activations)\n",
    "    d_output = E * slope_output_layer\n",
    "    Error_at_hidden_layer = torch.mm(d_output, wout.t())\n",
    "    d_hiddenlayer = Error_at_hidden_layer * slope_hidden_layer\n",
    "    wout += torch.mm(hidden_layer_activations.t(), d_output) *lr_rate\n",
    "    bout += d_output.sum() *lr_rate\n",
    "    wh += torch.mm(X.t(), d_hiddenlayer) *lr_rate\n",
    "    bh += d_output.sum() *lr_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result of our network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual :\n",
      " tensor([[1.],\n",
      "        [1.],\n",
      "        [0.]]) \n",
      "\n",
      "predicted :\n",
      " tensor([[0.9216],\n",
      "        [0.9177],\n",
      "        [0.1169]])\n"
     ]
    }
   ],
   "source": [
    "print('actual :\\n', y, '\\n')\n",
    "print('predicted :\\n', output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how we can build and train a neural network from scratch in PyTorch. Let’s now take things up a notch and dive into a case study. We will try to solve that case study using the techniques we have learned in this article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving an Image Classification Problem using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on following guide: https://www.analyticsvidhya.com/blog/2019/10/building-image-classification-models-cnn-pytorch/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our task is to identify the type of apparel by looking at a variety of apparel images. It’s a classic image classification problem using computer vision. This dataset, taken from the DataHack Platform, can be downloaded here: https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-apparels/?utm_source=blog&utm_medium=introduction-to-pytorch-from-scratch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 70,000 images, out of which 60,000 are in the training set and the remaining 10,000 in the test set. All the images are grayscale images of size (28*28).\n",
    "\n",
    "The dataset contains two folders – one each for the training set and the test set. In each folder, there is a .csv file that has the id of the image and its corresponding label and a folder containing the images for that particular set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# for reading and displaying images\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# for creating validation set\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# for evaluating the model\n",
    "from sklearn.metrics import accuracy_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "from torch.autograd import Variable\n",
    "from torch.nn import Linear, ReLU, CrossEntropyLoss, Sequential, Conv2d, MaxPool2d, Module, Softmax, BatchNorm2d, Dropout\n",
    "from torch.optim import Adam, SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  label\n",
       "0   1      9\n",
       "1   2      0\n",
       "2   3      0\n",
       "3   4      3\n",
       "4   5      0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading dataset\n",
    "train = pd.read_csv('/scratch/jonatank/data/apparel_comp/train_LbELtWX/train.csv')\n",
    "test = pd.read_csv('/scratch/jonatank/data/apparel_comp/test_ScVgIM0/test.csv')\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission_I5njJSF.csv')\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'id' here represents the name of the image (we just have to add .png as the images are in png format) and 'label 'is the corresponding class of that particular image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will load all the training images using the train.csv file. We will use a for loop to read all the images from the training set and finally store them as a NumPy array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "# loading training imgs\n",
    "train_img = np.zeros((np.count_nonzero(train['id']),28,28))\n",
    "\n",
    "# loop through every train img and load them to train_img numpy array\n",
    "for i, img_name in enumerate(train['id']):\n",
    "    img_path = '/scratch/jonatank/data/apparel_comp/train_LbELtWX/train/' + str(img_name) + '.png'\n",
    "    img = imread(img_path, as_gray=True)\n",
    "    img /= 255.0 # normaliz pixel values\n",
    "    img = img.astype('float32') # convert to float 32\n",
    "    train_img[i,:,:] = img\n",
    "    \n",
    "print(train_img.shape)\n",
    "\n",
    "# Create target for training the model\n",
    "train_img_y = train['label'].values\n",
    "print(train_img_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f280a94af90>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# random number generator\n",
    "seed = 123\n",
    "rng = np.random.RandomState(seed)\n",
    "\n",
    "# choose a random index\n",
    "rand_idx = np.random.randint(train_img.shape[0], size=1)[0]\n",
    "\n",
    "# print an image\n",
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(train_img[rand_idx,:,:] , cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((54000, 28, 28), (54000,)) ((6000, 28, 28), (6000,))\n"
     ]
    }
   ],
   "source": [
    "# create validation set with train_test_split()\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_img, train_img_y, test_size = 0.1)\n",
    "\n",
    "print((train_x.shape, train_y.shape), (val_x.shape, val_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have kept 10% data in the validation set and the remaining in the training set. Next, let’s convert the images and the targets into torch format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([54000, 1, 28, 28]) torch.Size([54000])\n",
      "torch.Size([6000, 1, 28, 28]) torch.Size([6000])\n"
     ]
    }
   ],
   "source": [
    "# convert training image numpy array to pytorch\n",
    "train_x = train_x.reshape(train_x.shape[0],1,train_x.shape[1],train_x.shape[2])\n",
    "train_x = torch.from_numpy(train_x)\n",
    "\n",
    "train_y = train_y.astype(int)\n",
    "train_y = torch.from_numpy(train_y)\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "\n",
    "# also we convert validation set\n",
    "val_x = val_x.reshape(val_x.shape[0],1,val_x.shape[1],val_x.shape[2])\n",
    "val_x = torch.from_numpy(val_x)\n",
    "\n",
    "val_y = val_y.astype(int)\n",
    "val_y = torch.from_numpy(val_y)\n",
    "\n",
    "print(val_x.shape, val_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define architecture\n",
    "class Net(Module):   \n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.cnn_layers = Sequential(\n",
    "            # Defining a 2D convolution layer\n",
    "            Conv2d(1, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "            # Defining another 2D convolution layer\n",
    "            Conv2d(4, 4, kernel_size=3, stride=1, padding=1),\n",
    "            BatchNorm2d(4),\n",
    "            ReLU(inplace=True),\n",
    "            MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "\n",
    "        self.linear_layers = Sequential(\n",
    "            Linear(4 * 7 * 7, 10)\n",
    "        )\n",
    "\n",
    "    # Defining the forward pass    \n",
    "    def forward(self, x):\n",
    "        x = self.cnn_layers(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.linear_layers(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s now call this model, and define the optimizer and the loss function for the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (cnn_layers): Sequential(\n",
      "    (0): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace=True)\n",
      "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (4): Conv2d(4, 4, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): ReLU(inplace=True)\n",
      "    (7): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (linear_layers): Sequential(\n",
      "    (0): Linear(in_features=196, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# defining the model\n",
    "model = Net()\n",
    "# defining the optimizer\n",
    "optimizer = Adam(model.parameters(), lr=0.07)\n",
    "# defining the loss function\n",
    "criterion = CrossEntropyLoss()\n",
    "# checking if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the architecture of the model. We have two Conv2d layers and a Linear layer. Next, we will define a function to train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    # getting the training set\n",
    "    x_train, y_train = Variable(train_x), Variable(train_y)\n",
    "    # getting the validation set\n",
    "    x_val, y_val = Variable(val_x), Variable(val_y)\n",
    "    # converting the data into GPU format\n",
    "    if torch.cuda.is_available():\n",
    "        x_train = x_train.cuda()\n",
    "        y_train = y_train.cuda()\n",
    "        x_val = x_val.cuda()\n",
    "        y_val = y_val.cuda()\n",
    "\n",
    "    # clearing the Gradients of the model parameters\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # prediction for training and validation set\n",
    "    output_train = model(x_train.float())\n",
    "    output_val = model(x_val.float())\n",
    "\n",
    "    # computing the training and validation loss\n",
    "    loss_train = criterion(output_train, y_train)\n",
    "    loss_val = criterion(output_val, y_val)\n",
    "    train_losses.append(loss_train)\n",
    "    val_losses.append(loss_val)\n",
    "\n",
    "    # computing the updated weights of all the model parameters\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "    tr_loss = loss_train.item()\n",
    "    if epoch%2 == 0:\n",
    "        # printing the validation loss\n",
    "        print('Epoch : ',epoch+1, '\\t', 'loss :', loss_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will train the model for 25 epochs and store the training and validation losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :  1 \t loss : tensor(2.4563, grad_fn=<NllLossBackward>)\n",
      "Epoch :  3 \t loss : tensor(3.6778, grad_fn=<NllLossBackward>)\n",
      "Epoch :  5 \t loss : tensor(2.3732, grad_fn=<NllLossBackward>)\n",
      "Epoch :  7 \t loss : tensor(1.8437, grad_fn=<NllLossBackward>)\n",
      "Epoch :  9 \t loss : tensor(1.9036, grad_fn=<NllLossBackward>)\n",
      "Epoch :  11 \t loss : tensor(1.9239, grad_fn=<NllLossBackward>)\n",
      "Epoch :  13 \t loss : tensor(1.7548, grad_fn=<NllLossBackward>)\n",
      "Epoch :  15 \t loss : tensor(1.5527, grad_fn=<NllLossBackward>)\n",
      "Epoch :  17 \t loss : tensor(1.3450, grad_fn=<NllLossBackward>)\n",
      "Epoch :  19 \t loss : tensor(1.1542, grad_fn=<NllLossBackward>)\n",
      "Epoch :  21 \t loss : tensor(1.0464, grad_fn=<NllLossBackward>)\n",
      "Epoch :  23 \t loss : tensor(1.0583, grad_fn=<NllLossBackward>)\n",
      "Epoch :  25 \t loss : tensor(0.9613, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "# defining the number of epochs\n",
    "n_epochs = 25\n",
    "# empty list to store training losses\n",
    "train_losses = []\n",
    "# empty list to store validation losses\n",
    "val_losses = []\n",
    "# training the model\n",
    "for epoch in range(n_epochs):\n",
    "    train(epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets plot training and validation loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting the training and validation loss\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(val_losses, label='Validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can clearly see that the training and validation losses are in sync. It is a good sign as the model is generalizing well on the validation set.\n",
    "\n",
    "Let’s check the accuracy of the model on the training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6675555555555556"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction for training set\n",
    "with torch.no_grad():\n",
    "    output = model(train_x.float())\n",
    "    \n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.numpy())\n",
    "predictions = np.argmax(prob, axis=1)\n",
    "\n",
    "# accuracy on training set\n",
    "accuracy_score(train_y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generating predictions for the test set\n",
    "We will load all the images in the test set, do the same pre-processing steps as we did for the training set and finally generate predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10000, 1, 28, 28])\n"
     ]
    }
   ],
   "source": [
    "# loading training imgs\n",
    "test_img = np.zeros((np.count_nonzero(test['id']),28,28))\n",
    "\n",
    "# loop through every train img and load them to train_img numpy array\n",
    "for i, img_name in enumerate(test['id']):\n",
    "    img_path = 'test_ScVgIM0/test/' + str(img_name) + '.png'\n",
    "    img = imread(img_path, as_gray=True)\n",
    "    img /= 255.0 # normaliz pixel values\n",
    "    img = img.astype('float32') # convert to float 32\n",
    "    test_img[i,:,:] = img\n",
    "    \n",
    "# convert list to torch tensor\n",
    "test_x = test_img.reshape(test_img.shape[0],1,test_img.shape[1],test_img.shape[2])\n",
    "test_x = torch.from_numpy(test_x)\n",
    "print(test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating predictions for test set\n",
    "with torch.no_grad():\n",
    "    output = model(test_x.float())\n",
    "\n",
    "softmax = torch.exp(output).cpu()\n",
    "prob = list(softmax.numpy())\n",
    "predictions = np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>60001</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>60002</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60003</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60004</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>60005</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  label\n",
       "0  60001      9\n",
       "1  60002      2\n",
       "2  60003      1\n",
       "3  60004      1\n",
       "4  60005      6"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# replacing the label with prediction\n",
    "sample_submission['label'] = predictions\n",
    "sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This gave us a score of <b> 0.67025 </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementation with fastai and resnet32 \n",
    "See kaggle kernel: https://www.kaggle.com/jkronander/identify-the-apparels-competition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With score <b>0.936</b> placing us <b>21</b> on scoreboard using fastai to train with resnet50."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
